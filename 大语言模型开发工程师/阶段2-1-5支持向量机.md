# **支持向量机（SVM）系统化学习资料**

---

# **1. 概念讲解**

## **1.1 SVM 的基本思想与背景**

支持向量机（Support Vector Machine, SVM）是一类用于 **二分类** 的监督学习方法，其核心目标是：

> **寻找一个能够最大化两类样本间“间隔（margin）”的分割超平面，使模型具有最强的泛化能力。**

简单来说：
在所有能区分两类数据的直线（二维）、平面（三维）或超平面（更高维）中，SVM 选择距离两类样本最远的那一条，这样模型面对新样本时更稳健、不易被噪声影响。

---

## **1.2 最大间隔分类思想**

“间隔”是指：

> **超平面到离它最近的样本点（这些点称为支持向量）的距离。**

最大间隔思想即：

* 不仅要能正确分类
* 还要让“最危险的点”（支持向量）尽量远离超平面

这样得到的超平面具有最好的泛化能力。

---

## **1.3 超平面的定义**

SVM 中的分类超平面用公式表示为：

[
w^T x + b = 0
]

其中：

* ( w )：法向量，决定超平面的方向
* ( b )：偏置，决定超平面在空间的位置
* ( x )：输入特征向量

超平面的作用：将样本划分为两类：

[
w^T x + b > 0 \quad \text{属于正类}
]
[
w^T x + b < 0 \quad \text{属于负类}
]

---

## **1.4 间隔（Margin）的定义**

几何间隔定义为：

> **超平面到支持向量的垂直距离。**

若某支持向量 (x_s) 满足：

[
y_s (w^T x_s + b) = 1
]

则几何间隔是：

[
\gamma = \frac{1}{|w|}
]

所以最大化间隔等价于：

> **最小化 ( |w| )**

---

## **1.5 核函数概念**

当数据在原空间无法线性分割时，SVM 通过核函数将数据隐式映射到高维空间，使其变得可线性分割。

但 SVM 并不直接计算高维向量，而是使用：

> **核函数计算高维空间中的内积，避免高维计算开销。**

---

## **1.6 常用核函数**

SVM 常见核函数包括：

### **1. 线性核**

[
K(x, x') = x^T x'
]

### **2. RBF（高斯）核**

[
K(x, x') = \exp(-\gamma |x - x'|^2)
]

### **3. 多项式核**

[
K(x, x') = (x^T x' + c)^d
]

---

## **1.7 软间隔**

真实数据往往无法完全线性分割，因此引入：

> **松弛变量 (\xi_i)**
> **允许部分点违反间隔甚至被误分类**

并通过参数 (C) 控制罚力度：

* (C) 大 ⇒ 强调“少犯错”
* (C) 小 ⇒ 强调“间隔更大”

---

# **2. 理论分析**

## **2.1 最大间隔优化目标**

在硬间隔条件下：

[
\min_{w,b} \frac{1}{2}|w|^2
]

约束：

[
y_i(w^T x_i + b) \ge 1
]

即所有样本都要在间隔边界外。

---

## **2.2 软间隔优化目标**

引入松弛变量 (\xi_i)：

[
y_i(w^T x_i + b) \ge 1 - \xi_i
]

优化目标：

[
\min_{w,b,\xi} \frac{1}{2}|w|^2 + C\sum_i \xi_i
]

意义：

* 第一项：尽量使间隔大
* 第二项：惩罚间隔违背，即误分类

---

## **2.3 核技巧数学直觉**

在高维空间中，线性可分性更容易实现。但直接映射到高维代价极大。核函数通过：

[
K(x, x') = \phi(x)^T \phi(x')
]

直接提供“高维内积”，无需显式计算 (\phi(x))。

核技巧优势：

1. 计算高效
2. 可构建极高维甚至无限维特征空间
3. 保持优化问题仍为凸问题

---

# **3. 示例演示（虚构二维示例）**

## **3.1 构造简单二维数据（虚构）**

假设有点集：

* 正类（+1）
  (1, 2), (2, 3), (2, 1)

* 负类（-1）
  (4, 4), (5, 5), (4, 3)

观察上图可发现类之间大致线性可分。

---

## **3.2 线性可分时的支持向量**

若找到超平面：

[
w^T x + b = 0
]

示意：

* 最靠近超平面的正类点可能是 (2, 1)
* 最靠近超平面的负类点可能是 (4, 3)

这两个点就是支持向量。

---

## **3.3 核函数效果对比（示意描述）**

### **线性核**

分割线接近一条直线，符合上述数据形状。

### **RBF 核**

若数据非线性（例如围成环状），RBF 核可产生弯曲边界。

### **多项式核**

可产生多段式或弧线型边界，复杂度取决于多项式阶数。

---

# **4. 小结**

SVM 的核心思想可总结为：

## **（1）最大间隔**

在所有可行的超平面中选择使两类数据间隔最大的那个，提升泛化能力。

## **（2）核技巧**

通过核函数将数据映射到高维，使线性不可分的问题变为线性可分。

## **（3）软间隔**

允许部分误分类，增强模型对噪声数据的容忍度，提高泛化能力。

三者关系如下：

* **最大间隔**是 SVM 的目标
* **核函数**帮助在高维实现线性分割
* **软间隔**处理真实数据中的噪声与不可分情况

---

# **5. Python 代码示例（结构示例，不可运行）**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

# ------------------------------------------
# 1. 构造虚构数据（二维）
# ------------------------------------------

# 正类 (+1)
X_pos = np.array([
    [1, 2],
    [2, 3],
    [2, 1]
])

# 负类 (-1)
X_neg = np.array([
    [4, 4],
    [5, 5],
    [4, 3]
])

# 合并
X = np.vstack([X_pos, X_neg])
y = np.array([1, 1, 1, -1, -1, -1])

# ------------------------------------------
# 2. 训练线性 SVM（可切换 kernel）
# ------------------------------------------

model = SVC(kernel="linear", C=1.0)
model.fit(X, y)

# ------------------------------------------
# 3. 打印支持向量
# ------------------------------------------

print("支持向量：")
print(model.support_vectors_)

print("\n支持向量索引：")
print(model.support_)

print("\n支持向量对应的类别：")
print(y[model.support_])

# ------------------------------------------
# 4. 可视化结果（线性可分示例）
# ------------------------------------------

plt.figure(figsize=(6, 6))

# 画散点
plt.scatter(X_pos[:, 0], X_pos[:, 1], color='blue', label='Positive (+1)')
plt.scatter(X_neg[:, 0], X_neg[:, 1], color='red', label='Negative (-1)')

# 画支持向量（加圈）
plt.scatter(model.support_vectors_[:, 0],
            model.support_vectors_[:, 1],
            s=200, facecolors='none', edgecolors='black', label='Support Vectors')

# 画分类超平面
w = model.coef_[0]
b = model.intercept_[0]

# 构造坐标范围
x_line = np.linspace(0, 6, 100)

# 超平面公式 w1*x + w2*y + b = 0 → y = -(w1*x + b)/w2
y_line = -(w[0] * x_line + b) / w[1]

# 正负间隔线：y = -(w1*x + b ± 1)/w2
y_margin_up = -(w[0] * x_line + b - 1) / w[1]
y_margin_down = -(w[0] * x_line + b + 1) / w[1]

plt.plot(x_line, y_line, 'k-', label='Decision Boundary')
plt.plot(x_line, y_margin_up, 'k--')
plt.plot(x_line, y_margin_down, 'k--')

plt.xlim(0, 6)
plt.ylim(0, 6)
plt.legend()
plt.title("SVM 线性分类示例（含支持向量）")
plt.show()
```

---

如需我继续生成 **习题、可视化示意图、推导细化、与其他算法对比** 等内容，也可以告诉我！
