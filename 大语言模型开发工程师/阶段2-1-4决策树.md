# **1. 介绍：决策树的背景与意义**

决策树（Decision Tree）是一种基本但非常重要的机器学习模型，常用于**分类（Classification）**和**回归（Regression）**任务。它借鉴了人类的思考方式：通过一连串“是/否”选择或条件判断来得到最终结论。

### **为什么决策树重要？**

* **可解释性强**：比神经网络等模型更容易理解。
* **无需特征标准化**：不像SVM、KNN，决策树不受特征尺度影响。
* **可处理数值与类别特征**。
* **广泛应用**于模式识别、客户决策分析、信用评估等领域。

### **决策树的基本构成**

一棵决策树通常由三类节点组成：

* **根节点（Root Node）**
* **内部节点（Internal Node）**
* **叶节点（Leaf Node）**

接下来我们将逐一讲解。

---

# **2. 树结构组成：根节点、内部节点、叶节点**

## **2.1 根节点（Root Node）**

### **定义**

根节点是决策树的起始点，代表整个数据集。

### **作用**

* 代表要分析的整体数据
* 决策树从根节点开始选择最优特征进行第一次划分

### **示例**

假设你要预测客户是否购买商品，根节点可能代表整个客户数据集，下一步根据“年龄”“收入”等特征来划分。

---

## **2.2 内部节点（Internal Node）**

### **定义**

内部节点用于根据特征进行数据划分。

### **作用**

* 表示某个特征的分裂条件（如“年龄 > 30”）
* 把数据分成多个子集

### **示例**

如果内部节点条件为“收入 > 8000”，则会把样本分为收入高和收入低的两部分。

---

## **2.3 叶节点（Leaf Node）**

### **定义**

叶节点表示决策结果。

### **作用**

* 在分类问题中输出类别（如“购买” / “不购买”）
* 在回归问题中输出预测值（如预测房价）

### **示例**

某个叶节点可能最终判断为“客户将会购买”。

---

# **3. 特征划分准则：信息增益（基于熵）**

决策树的关键步骤是**选择哪个特征去分裂数据**，而信息增益是常用的标准（ID3算法）。

---

## **3.1 信息增益（Information Gain）**

### **定义**

信息增益表示某个特征能够减少多少不确定性（熵）。

### **公式**

$$
IG(D, A) = Entropy(D) - \sum_{v \in A} \frac{|D_v|}{|D|} Entropy(D_v)
$$

含义：

* 分裂前的不确定性（熵）
* 减去
* 分裂后的加权熵

---

## **3.2 熵（Entropy）**

### **定义**

熵衡量数据的不确定性。

### **公式**

$$
Entropy(D) = - \sum_{i=1}^{k} p_i \log_2 p_i
$$

其中
$p_i$ 表示类别 i 的概率

### **示例**

如果一个数据集中：

* 购买：80%
* 不购买：20%

熵就比较低（更纯）
如果：

* 购买：50%
* 不购买：50%

熵最高（最混乱）

---

## **3.3 实例：选择最佳特征**

假设你有两个特征：

* 年龄
* 收入

通过计算每个特征的信息增益，可以找到最能降低不确定性的特征用于分裂。

---

# **4. 基尼指数：衡量节点纯度的指标（用于 CART）**

CART 决策树使用基尼指数（Gini Index）。

---

## **4.1 定义**

基尼指数越小，数据越纯。

### **公式**

$$
Gini(D) = 1 - \sum_{i=1}^{k} p_i^2
$$

---

## **4.2 示例**

如果：

* 类 A：50%
* 类 B：50%

则：

$$
Gini = 1 - (0.5^2 + 0.5^2) = 0.5
$$

值较高 → 数据混杂

如果：

* 类 A：90%
* 类 B：10%

$$
Gini = 1 - (0.9^2 + 0.1^2) = 0.18
$$

值低 → 更纯

---

# **5. 递归划分过程：逐层选择最优特征**

这是建树核心，决策树通过**递归**不断选择最佳特征分裂数据。

---

## **步骤**

1. **从根节点开始**，计算所有特征的信息增益或基尼指数
2. **选择最佳特征**进行分裂
3. 对每个子数据集重复这个过程
4. **直到满足停止条件为止**

---

## **示例**

第一次分裂：

* 选择“收入”作为分裂特征

第二次分裂：

* 在“高收入”子集下，再选择“年龄”

直到每个叶节点的数据纯度足够高。

---

# **6. 停止条件：深度限制、最小样本数、纯度阈值**

防止决策树过度生长（过拟合）。

---

## **常见停止条件**

### 1. **最大树深度（max_depth）**

树太深，会过拟合。

### 2. **最小样本数（min_samples_split）**

当样本数太少，不再继续分裂。

### 3. **纯度阈值**

如果节点熵或基尼指数足够小，就停止划分。

---

## **示例**

设置：

* 最大深度 = 5
* 节点最小样本数 = 10

当树达到 5 层或节点样本 < 10 时，停止分裂。

---

# **7. 剪枝方法：预剪枝与后剪枝**

剪枝用于减少模型复杂度，防止过拟合。

---

# **7.1 预剪枝（Pre-pruning）**

### **定义**

在建树过程中提前停止分裂。

### **方法**

* 限制深度
* 限制最小样本数
* 限制纯度阈值

优点：训练更快
缺点：可能错过更优的树结构

---

# **7.2 后剪枝（Post-pruning）**

### **定义**

先构建完整的树，再删除不必要的分支。

### **方法**

* 通过交叉验证比较剪枝前后性能
* 若删除某分支后性能提升，则保留剪枝

优点：效果更好
缺点：训练时间更长

---

## **示例**

完整决策树有一个分支对预测作用不大，通过后剪枝可以将其删除，使树更简洁。

---

# **8. 总结**

* 决策树通过一系列条件判断实现分类或回归
* 根节点 → 内部节点 → 叶节点构成层次结构
* 最优特征选择基于**信息增益**（ID3）、**信息增益率**（C4.5）、**基尼指数**（CART）
* 建树过程依赖递归划分
* 必须设置停止条件避免过拟合
* 使用剪枝（预剪枝 + 后剪枝）优化模型

决策树简单易懂，但若不加限制容易过拟合，所以剪枝非常关键。

---

# **代码示例**

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import tree
import matplotlib.pyplot as plt

# 加载数据集
data = load_iris()
X = data.data
y = data.target

# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建决策树模型
clf = DecisionTreeClassifier(criterion='entropy')  # 使用信息增益作为分裂准则
clf.fit(X_train, y_train)

# 预测与评估
y_pred = clf.predict(X_test)
accuracy = np.mean(y_pred == y_test)
print(f'模型准确率: {accuracy:.2f}')

# 可视化决策树
plt.figure(figsize=(10,8))
tree.plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names)
plt.show()
```