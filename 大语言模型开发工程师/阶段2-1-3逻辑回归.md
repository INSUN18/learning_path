# 逻辑回归学习材料

逻辑回归（Logistic Regression）是机器学习中一种常见的分类算法，尤其适用于二分类问题。它不仅可以帮助我们判断一个样本属于哪个类别，还能预测其属于某一类别的概率。以下内容将系统地介绍逻辑回归的相关知识，帮助学生从基础到复杂逐步掌握这一算法。

## 3. 逻辑回归（Logistic Regression）

### 3.1 二分类目标：预测样本属于正类的概率

#### 概念讲解

在二分类问题中，我们的目标是预测一个样本属于某一类（通常为正类，记为1）的概率，而不是直接预测该样本属于哪个类别。二分类问题中，样本要么属于正类（标记为1），要么属于负类（标记为0）。在实际应用中，很多时候我们希望得到样本属于正类的概率值，这样可以更加灵活地进行决策。例如，如果某个样本的正类概率为0.7，我们可能会根据此概率来选择是否将其归为正类或负类。

#### 理论分析

逻辑回归的核心目标是估计样本属于正类的概率。对于样本 ( x )，逻辑回归输出一个介于 0 和 1 之间的值 ( P(y=1|x) )，表示样本 ( x ) 属于正类的概率。通过这种方式，逻辑回归能够进行概率估计，方便我们在决策中使用不同的阈值来做出不同的分类决策。

### 3.2 Sigmoid 函数：σ(z) = 1 / (1 + e⁻ᶻ)

#### 概念讲解

Sigmoid 函数（也叫做逻辑函数）是逻辑回归的核心部分，它能够将输入值 ( z ) 映射到 0 和 1 之间。Sigmoid 函数的数学形式如下：

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

其中，( z ) 是逻辑回归模型的输出值，它通常是一个线性组合，即 ( z = w^T x + b )，其中 ( w ) 是权重向量，( x ) 是输入特征向量，( b ) 是偏置。

#### Sigmoid 函数的作用

Sigmoid 函数的作用是将线性回归的输出（可能是任何实数）转换成一个在 0 到 1 之间的概率值。逻辑回归的输出正是通过 Sigmoid 函数计算得到的 ( P(y=1|x) )，表示样本 ( x ) 属于正类的概率。

#### 数学推导

Sigmoid 函数的导数为：

[
\sigma'(z) = \sigma(z) (1 - \sigma(z))
]

这对于优化过程至关重要，因为我们将在梯度下降中使用这个导数来更新模型参数。可以看到，Sigmoid 函数的导数是一个关于输出值的函数，其形状与Sigmoid函数本身紧密相关。

#### 比较与其他激活函数

Sigmoid 函数与其他激活函数（如 ReLU）相比，最大的不同是它的输出范围是固定的 [0, 1]，非常适合用于概率预测。而像 ReLU 函数，虽然在某些任务中表现更好，但它的输出没有界限，可能会导致梯度爆炸的问题。

### 3.3 逻辑回归模型形式：P(y=1|x) = σ(wᵀx + b)

#### 概念讲解

逻辑回归的核心模型形式为：

[
P(y=1|x) = \sigma(w^T x + b)
]

其中：

* ( x ) 是输入特征向量。
* ( w ) 是权重向量。
* ( b ) 是偏置项。

该公式表示样本 ( x ) 属于正类的概率。通过 Sigmoid 函数对 ( w^T x + b ) 进行变换，得到一个介于 0 和 1 之间的概率值。

#### 实例演示

假设有一个样本 ( x = [1.5, 2.0] )，权重 ( w = [0.4, -0.6] )，偏置 ( b = 0.2 )，那么线性组合 ( w^T x + b = 0.4(1.5) + (-0.6)(2.0) + 0.2 = 0.6 - 1.2 + 0.2 = -0.4 )。

接着，通过 Sigmoid 函数计算该值对应的概率：

[
P(y=1|x) = \frac{1}{1 + e^{-(-0.4)}} \approx 0.4013
]

这意味着样本 ( x ) 属于正类的概率约为 0.4。

#### 数学推导

通过 Sigmoid 函数的性质，我们知道它可以将任意实数值转化为 0 到 1 之间的概率。对于逻辑回归来说，这一过程非常重要，因为我们最终需要通过概率来进行分类决策。

### 3.4 对数似然损失函数：最大化样本实际标签的对数似然

#### 概念讲解

对数似然损失函数是逻辑回归的损失函数，它反映了模型输出的概率值与实际标签之间的差距。通过最大化对数似然函数，我们能够得到最优的模型参数。

对数似然损失函数的形式为：

[
L(w, b) = -\sum_{i=1}^{m} \left[ y^{(i)} \log(P(y=1|x^{(i)})) + (1 - y^{(i)}) \log(1 - P(y=1|x^{(i)})) \right]
]

其中 ( m ) 是样本数量，( y^{(i)} ) 是第 ( i ) 个样本的实际标签，( P(y=1|x^{(i)}) ) 是模型对第 ( i ) 个样本的预测概率。

#### 理论分析

逻辑回归的目标是通过最大化对数似然函数来找到最优的模型参数 ( w ) 和 ( b )。与均方误差损失不同，对数似然损失函数专门针对概率预测设计，它能更好地衡量预测概率与真实标签的差异。

#### 数学推导

对数似然损失函数的推导涉及到条件概率和对数函数的性质，最终得出上述损失函数形式。我们通过最小化该损失函数（即最大化对数似然）来求解模型参数。

#### 实例演示

在实际应用中，我们通过计算对数似然损失值来优化模型参数。一般来说，我们使用梯度下降等优化方法来更新权重和偏置，以最小化损失函数。

### 3.5 梯度下降求解：通过反向传播更新权重

#### 概念讲解

梯度下降是优化模型参数的常用方法。在逻辑回归中，我们通过梯度下降最小化损失函数，逐步更新模型的权重和偏置。

#### 理论分析

梯度下降的核心思想是根据损失函数的梯度（即导数）来调整参数，以最小化损失。每次调整参数的幅度由学习率控制，学习率过大可能会导致震荡，过小则会导致收敛过慢。

#### 数学推导

通过对损失函数进行求导，我们可以得到权重和偏置的更新公式：

[
w := w - \eta \frac{\partial L}{\partial w}
]

[
b := b - \eta \frac{\partial L}{\partial b}
]

其中，( \eta ) 是学习率，( \frac{\partial L}{\partial w} ) 和 ( \frac{\partial L}{\partial b} ) 是损失函数对权重和偏置的偏导数。

#### 代码示例

以下是一个简单的 Python 代码示例，展示了如何使用梯度下降训练逻辑回归模型：

```python
import numpy as np

# Sigmoid函数
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 损失函数：对数似然
def compute_loss(X, y, w, b):
    m = len(y)
    predictions = sigmoid(np.dot(X, w) + b)
    loss = -(1/m) * np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))
    return loss

# 梯度下降更新权重
def gradient_descent(X, y, w, b, learning_rate, iterations):
    m = len(y)
    for i in range(iterations):
        predictions = sigmoid(np.dot(X, w) + b)
        dw = (1/m) * np.dot(X.T, predictions - y)
        db = (1/m) * np.sum(predictions - y)
        
        # 更新权重和偏置
        w -= learning_rate * dw
        b -= learning_rate * db
        
        # 打印每100步的损失
        if i % 100 == 0:
            loss = compute_loss(X, y, w, b)
            print(f"Iteration {i}, Loss: {loss}")
    return w, b

# 示例数据：X为特征矩阵，y为标签
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])

# 初始化权重和偏置
w = np.zeros(X.shape[1])
b = 0

# 学习率和迭代次数
learning_rate = 0.1
iterations = 1000

# 训练逻辑回归模型
w, b = gradient_descent(X, y, w, b, learning_rate, iterations)

# 最终权重和偏置
print(f"Trained weights: {w}, Trained bias: {b}")
```

### 3.6 决策边界：由 wᵀx + b = 0 定义的分界线

#### 概念讲解

逻辑回归的决策边界是指将样本空间划分为正类和负类的界限。决策边界的方程为 ( w^T x + b = 0 )，这是一个超平面（在二维空间中是直线），它将样本空间分为两部分。

#### 理论分析

从模型的线性形式 ( w^T x + b = 0 ) 出发，决策边界就是通过这个方程找到的超平面，它划定了正类和负类的分界线。

#### 实例演示

通过可视化二维数据集，可以直观地展示决策边界的位置，帮助学生理解如何根据决策边界来进行分类。

#### 数学推导

决策边界方程 ( w^T x + b = 0 ) 可以看作是通过模型输出为 0.5 的地方。在实际操作中，决策边界的推导涉及到模型输出的概率与阈值的关系。

---

通过本材料的学习，学生应能理解逻辑回归的工作原理、优化过程及其应用，并能通过实际代码实现一个简单的逻辑回归模型。
