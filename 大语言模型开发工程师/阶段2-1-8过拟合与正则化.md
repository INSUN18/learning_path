# **1. 引言：为什么我们要讨论过拟合与正则化？**

在机器学习中，一个模型能否“记住”训练数据其实并不重要；真正重要的是它能否在**从未见过的新数据**上仍然做出正确的预测，这种能力叫作 **泛化能力（Generalization）**。

但现实中我们经常会遇到两类问题：

* 模型把训练数据记得“太牢” → **过拟合**
* 模型压根没学到东西 → **欠拟合**

本章将带你从零开始理解：

1. 过拟合和欠拟合是什么？为什么会发生？
2. 正则化如何帮助我们控制模型复杂度？
3. 提前停止为什么能防止模型“学过头”？
4. 如何通过实验证明这些现象？

请你把这一章当作构建“泛化能力”认知体系的入口。

---

# **2. 基本概念解析**

## **2.1 过拟合：模型学得太“细”了**

过拟合是指一个模型把训练数据中的**噪声**也当成了规律，因此在训练集表现很好，但在新数据上表现很差。

你可以把它想象成：
学生把课堂例题背得滚瓜烂熟，但一到考试遇到新题就不会做。

**过拟合的典型特征：**

* 训练误差：很低
* 验证/测试误差：反而很高
* 拟合曲线通常会出现剧烈震荡

---

## **2.2 欠拟合：模型学得太“粗”了**

欠拟合说明模型太简单，无法抓住数据的真实趋势。

比如，用一条直线去拟合明显弯曲的曲线数据。

**欠拟合的特征：**

* 训练误差高
* 验证误差也高
* 模型表达能力不足

---

## **2.3 为什么要正则化？**

正则化（Regularization）的本质是：
**给模型的参数加“约束”，让模型不要太自由。**

你可以把它理解为“提醒模型保持克制”：
不要随便给每个特征都学一个复杂的权重。

**正则化的作用：**

* 降低过拟合的风险
* 提升模型泛化能力
* 让模型参数更稳定

---

## **2.4 L1 正则化：让模型自动“选特征”**

L1 正则化的惩罚项是：

[
\lambda\sum_i |w_i|
]

它的特点是：

* 会让某些权重变成 0
* 产生“稀疏性”
* 相当于自动做特征选择

所以 L1 很适合特征很多、但只有少量重要特征的场景。

---

## **2.5 L2 正则化：让模型更平滑、参数更稳定**

L2 正则化的惩罚项是：

[
\lambda\sum_i w_i^2
]

特点：

* 不会产生稀疏
* 会让所有权重变小
* 模型更加平滑（不容易过度弯曲）
* 许多模型默认用它，如神经网络的 weight decay

---

## **2.6 提前停止（Early Stopping）：监控验证误差，避免“学过头”**

随着训练的进行：

* 训练误差会一直下降
* 验证误差通常会先降后升

当验证误差开始上升，说明模型已经开始记住噪声 → 进入过拟合阶段。

提前停止的做法是：

1. 训练时记录每个 epoch 的验证误差
2. 一旦验证误差连续上升
3. **立即停下来**

它是一种非常有效的“训练过程上的正则化方法”。

---

# **3. 理论分析：从偏差—方差角度理解**

## **3.1 偏差—方差分解：解释过拟合与欠拟合的理论框架**

机器学习模型的期望误差可以表示为：

[
\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Noise}
]

这三项分别意味着：

* **偏差（Bias）高 → 欠拟合**
  模型太简单，根本没学到数据规律。

* **方差（Variance）高 → 过拟合**
  模型太复杂，对训练数据过度敏感。

正则化的作用就是：

* 略微增加偏差
* 显著降低方差
* 从而让整体误差更低

因此，正则化是一个**“以小换大”**的策略。

---

## **3.2 L1 / L2 如何体现在目标函数中？**

假设原始损失为：

[
\mathcal{L}(w)
]

加入正则化后：

### **L1（Lasso）损失：**

[
\mathcal{L}_{L1}(w)=\mathcal{L}(w)+\lambda \sum_i |w_i|
]

它能让权重精确变成 0 → 稀疏模型。

### **L2（Ridge）损失：**

[
\mathcal{L}_{L2}(w)=\mathcal{L}(w)+\lambda \sum_i w_i^2
]

它让所有权重都变小 → 曲线更平滑。

---

## **3.3 提前停止与验证误差曲线**

典型曲线变化：

* 初期：训练误差 ↓，验证误差 ↓
* 中期：验证误差达到最低点
* 后期：验证误差 ↑（开始过拟合）

提前停止通过检测验证误差的变化，防止训练越走越偏。

你可以把它看作一种 **“动态的正则化”**。

---

# **4. 实例演示：人工数据实验**

我们构造数据：

[
y=0.5x+\text{噪声}
]

并加入高次特征来制造过拟合。

---

## **4.1 无正则化：高阶模型容易过拟合**

特点：

* 拟合曲线出现剧烈震荡
* 训练误差很低，验证误差很差

---

## **4.2 加入 L1 正则化**

* 部分高次项的权重直接变为 0
* 模型自动变“简单”
* 曲线更平滑

---

## **4.3 加入 L2 正则化**

* 所有权重被压小
* 但不会变成 0
* 模型更稳定，不容易乱摆动

---

## **4.4 使用提前停止**

通过模拟训练过程观察：

* 验证误差最低点附近是最佳模型
* 提前停止可以避免后期的过拟合

---

# **5. 总结与学习建议**

## **5.1 知识结构一图式理解**

| 现象        | 本质问题       | 表现          | 解决办法          |
| --------- | ---------- | ----------- | ------------- |
| **过拟合**   | 模型太复杂（高方差） | 训练误差低，验证误差高 | 正则化、提前停止、减少特征 |
| **欠拟合**   | 模型太简单（高偏差） | 两者误差都高      | 增加特征、提升模型复杂度  |
| **L1 正则** | 控制复杂度      | 产生稀疏性       | 自动特征选择        |
| **L2 正则** | 控制复杂度      | 平滑权重        | 神经网络常用        |
| **提前停止**  | 防止训练过头     | 动态停止训练      | 高效常用          |

---

## **5.2 学习与实践建议**

* 多画训练曲线（train vs val）
* 默认先使用 **L2 正则**，再尝试 L1
* 神经网络训练必须考虑提前停止
* 多尝试不同 λ、不同特征组合
* 做实验时记录误差变化，这比公式更重要

---

# **6. 代码示例（Python）**

以下代码完整展示：

* 构造数据
* 无正则化 / L1 / L2 回归
* 手写版提前停止

（代码保持原格式，仅调整必要注释）

```python
import numpy as np
from sklearn.linear_model import LinearRegression, Lasso, Ridge
import matplotlib.pyplot as plt

# 1. 构造随机数据
np.random.seed(42)
X = np.linspace(-3, 3, 80).reshape(-1, 1)
y = 0.5 * X[:, 0] + np.random.randn(80) * 0.5

# 构造高次特征以制造过拟合风险
X_poly = np.concatenate([X**i for i in range(1, 6)], axis=1)

# 2. 训练不同模型
model_lr = LinearRegression().fit(X_poly, y)
model_l1 = Lasso(alpha=0.1).fit(X_poly, y)
model_l2 = Ridge(alpha=0.1).fit(X_poly, y)

# 3. 预测用于可视化
x_plot = np.linspace(-3, 3, 200).reshape(-1, 1)
x_plot_poly = np.concatenate([x_plot**i for i in range(1, 6)], axis=1)

y_lr = model_lr.predict(x_plot_poly)
y_l1 = model_l1.predict(x_plot_poly)
y_l2 = model_l2.predict(x_plot_poly)

# 4. 简易提前停止模拟
def train_with_early_stopping(X, y, max_epoch=200, patience=10, lr=0.001):
    np.random.seed(0)
    w = np.random.randn(X.shape[1])
    
    best_w = w.copy()
    best_loss = np.inf
    wait = 0

    for epoch in range(max_epoch):
        y_pred = X.dot(w)
        loss = np.mean((y - y_pred) ** 2)

        # 梯度下降
        grad = -2 * X.T.dot(y - y_pred) / len(y)
        w -= lr * grad
        
        # 早停逻辑
        if loss < best_loss:
            best_loss = loss
            best_w = w.copy()
            wait = 0
        else:
            wait += 1
            if wait >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
    return best_w

w_es = train_with_early_stopping(X_poly, y)
y_es = x_plot_poly.dot(w_es)

# 5. 可视化
plt.scatter(X, y, label="data")
plt.plot(x_plot, y_lr, label="No Regularization")
plt.plot(x_plot, y_l1, label="L1 Regularization")
plt.plot(x_plot, y_l2, label="L2 Regularization")
plt.plot(x_plot, y_es, label="Early Stopping")
plt.legend()
plt.show()
```

---
