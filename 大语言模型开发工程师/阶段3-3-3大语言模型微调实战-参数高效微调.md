# **3. 参数高效微调（PEFT / LoRA）**

---

# **3.1 LoRA 的低秩分解原理（Low-Rank Adaptation）**

---

## **1. 概念讲解**

LoRA（Low-Rank Adaptation）是一种 **参数高效微调方法**，其核心思想是：

> **冻结预训练模型所有原始权重，仅在特定层插入低秩可训练矩阵，以低成本学习任务特定的更新。**

传统微调需要更新上百亿参数，而 LoRA 通过“低秩分解”只引入极少量可训练参数（如 rank=8 时，减少 10,000 倍以上）。

---

## **2. 理论 / 数学分析**

### **（1）传统全量微调**

对于 Transformer 中某层权重矩阵：

[
W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}
]

全量微调会直接更新整个矩阵：

[
W' = W + \Delta W
]

但 (\Delta W) 参数量极大，不利于训练。

---

### **（2）LoRA 的低秩分解思想**

LoRA 认为任务特定的更新 (\Delta W) 在高维权重空间中实际处在一个低维子空间，可以分解为：

[
\Delta W = BA
]

其中：

* (A \in \mathbb{R}^{r \times d_{\text{in}}})
* (B \in \mathbb{R}^{d_{\text{out}} \times r})
* (r) 是一个很小的数（典型值：4, 8, 16）

即用 **rank=r 的低秩矩阵替代大矩阵更新**。

最终输出变为：

[
h = W x + BAx
]

**优势：**

* 参数量从 (d_\text{out} \times d_\text{in}) → (r(d_\text{out}+d_\text{in}))
* 训练更快
* 不破坏原模型能力（权重冻结）

---

### **（3）低秩分解的合理性**

预训练模型权重非常高维，其局部工件往往具有冗余性。
任务数据量有限时：

> **任务更新矩阵 (\Delta W) 本质上是低秩的，LoRA 刚好利用了这种低维结构。**

---

## **3. 配置示例（HuggingFace PEFT）**

```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=8,                    # rank = 8
    lora_alpha=16,          # scaling
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"],
)
```

---

## **4. 代码示例：在 Qwen2 微调中插入 LoRA 模块**

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

model_name = "Qwen/Qwen2-0.5B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
)

model = get_peft_model(model, config)
model.print_trainable_parameters()
```

输出会显示仅有几千个可训练 LoRA 参数。

---

## **5. 小结**

| 关键点       | 内容                  |
| --------- | ------------------- |
| LoRA 做了什么 | 为每个被选中的层插入低秩矩阵 BA   |
| 为什么有效     | 任务特定权重更新是低秩的        |
| 参数效率      | 参数量显著减少（可达 10000 倍） |
| 优势        | 加速、节省显存、稳定性高、可堆叠多任务 |

---

# **3.2 LoRA 的可训练参数选择（Target Modules）**

---

## **1. 概念讲解**

LoRA 不会作用于模型所有层。

> **用户必须指定哪些模块需要注入 LoRA。**

常见选择：

* Attention 的投影层（Q、K、V、O）
* FFN 的线性层（如 gate_proj、up_proj、down_proj）

不同模型模块名称不同，例如：

| 模型       | Attention 模块名称              |
| -------- | --------------------------- |
| LLaMA 系列 | q_proj、k_proj、v_proj、o_proj |
| Qwen 系列  | q_proj、v_proj（部分版本合并 K）     |
| ChatGLM  | query_key_value             |

---

## **2. 为什么只选部分层？**

### **（1）注意力层是影响模型行为的关键层**

注意力投影影响：

* 信息选择
* 上下文建模
* 解码行为

因此对任务迁移最有效。

### **（2）减少冗余参数**

例如 LLaMA-7B 的全量微调需要 7B 参数，而 attention 的 Q/V 投影只占 < 1%。

### **（3）避免影响模型基础能力**

选择局部层插入 LoRA 可确保：

* **语义能力不被破坏**
* **任务特定学习更集中**

---

## **3. 配置示例**

```python
lora_config = LoraConfig(
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
)
```

ChatGLM 示例：

```python
target_modules=["query_key_value"]
```

---

## **4. 代码示例：打印可选模块**

```python
for name, module in model.named_modules():
    if "proj" in name.lower():
        print(name)
```

你可以根据打印结果来确定需要添加 LoRA 的模块。

---

## **5. 小结**

| 内容       | 说明                         |
| -------- | -------------------------- |
| 为什么要选择模块 | 控制训练成本、保持模型稳定              |
| 默认选择     | Attention Q/V 投影层          |
| 更强微调     | 可以加入 FFN 的 up/down/gate 层  |
| 如何查看模块名  | 使用 `model.named_modules()` |

---

# **3.3 LoRA 的 Rank 超参数设定（r 值）**

---

## **1. 概念讲解**

LoRA 的 rank（r）决定低秩矩阵的维度，也是 LoRA 性能最重要的超参之一。

[
\Delta W = B A,\quad A \in \mathbb{R}^{r \times d_{\text{in}}},; B \in \mathbb{R}^{d_{\text{out}} \times r}
]

**r 越大 → 可学习能力越强 → 参数量也越大。**

---

## **2. r 的影响**

### **（1）性能提升**

r 越大，(\Delta W) 越接近全秩更新，有更强的拟合能力。

### **（2）显存开销**

LoRA 参数量：

[
\text{Params} = r (d_\text{in} + d_\text{out})
]

r 增大显存线性增加。

### **（3）过拟合风险**

高 r 会导致模型在小数据上过拟合。

---

## **3. r 的推荐设置**

| 任务类型      | 推荐 r  |
| --------- | ----- |
| 小样本 QA、分类 | 4~8   |
| 指令微调（SFT） | 8~16  |
| 复杂生成、对话   | 16~32 |
| 高性能训练     | 32+   |

> 对中文大模型 SFT，**r=8~16** 是最常见的最佳区间。

---

## **4. 实例配置**

```python
lora_config = LoraConfig(
    r=16,         # 可学习能力更强
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"]
)
```

---

## **5. 代码示例：比较不同 r 的参数量**

```python
from peft import LoraConfig, get_peft_model

for r in [4, 8, 16, 32]:
    cfg = LoraConfig(
        r=r,
        lora_alpha=2*r,
        target_modules=["q_proj", "v_proj"]
    )
    peft_model = get_peft_model(model, cfg)
    print(f"r={r}")
    peft_model.print_trainable_parameters()
```

---

## **6. 小结**

| 关键点         | 内容             |
| ----------- | -------------- |
| rank r 决定什么 | LoRA 的表达能力、参数量 |
| 影响          | 性能↑、显存↑、过拟合风险↑ |
| 推荐值         | 常用 8～16        |
| 经验法则        | 若显存足够，可适当提高 r  |

---

# **3.4 LoRA 的 Alpha 缩放参数设定（lora_alpha）**

---

## **1. 概念讲解**

LoRA 的最终更新公式是：

[
W' = W + \alpha \cdot \frac{1}{r} BA
]

其中：

* **r**：rank
* **B、A**：LoRA 可训练矩阵
* **α（lora_alpha）**：缩放因子

> **α 用于控制 LoRA 更新对原始权重的影响强度。**

如果不加缩放，高 r 会使模型更新幅度过大，导致不稳定。

---

## **2. 理论 / 数学分析**

为什么要引入 α？

[
\Delta W = \alpha \cdot \frac{1}{r} BA
]

观察期望值：

* rank 越大，(BA) 的值通常越大
* 使用 (\frac{1}{r}) 是为了**归一化更新幅度**
* 而 α 用于**重新调整规模，使其更贴近原始模型的更新尺度**

换句话说：

* (\frac{1}{r}) 是数学上的归一化
* α 是工程上的微调强度控制

---

## **3. α 的影响**

| α 较小       | α 较大       |
| ---------- | ---------- |
| LoRA 学习能力弱 | 学习能力强      |
| 不易过拟合      | 更易过拟合      |
| 微调效果可能不足   | 显存略增、训练更敏感 |

常见配置：

[
\alpha = 2r \quad \text{或} \quad \alpha = r
]

例如 r=8 → α=16 常见。

---

## **4. 推荐配置**

| rank r | 推荐 α    |
| ------ | ------- |
| 4      | 8 或 16  |
| 8      | 16 或 32 |
| 16     | 32 或 64 |
| >32    | 64+     |

经验法则：

> α 通常是 r 的 1～4 倍。

---

## **5. 配置示例**

```python
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,   # 推荐配置：alpha = 2 * r
    target_modules=["q_proj", "v_proj"]
)
```

---

## **6. 小结**

* α 用于平衡 LoRA 更新大小
* α 应与 r 成正比
* 常用 α = r 或 α = 2r
* 太小 → 学得不够；太大 → 不稳定

---

# **3.5 LoRA 的 Dropout 使用策略（lora_dropout）**

---

## **1. 概念讲解**

LoRA dropout 仅作用于 LoRA 分支：

[
\Delta W x = BA(D(x))
]

其中 D 是 dropout 运算。

作用：

> **缓解过拟合，提高 LoRA 泛化能力。**

---

## **2. Dropout 的数学解释**

dropout 以概率 (p) 随机置零部分输入：

[
D(x_i)=
\begin{cases}
0, & \text{with prob } p \
\frac{x_i}{1-p}, & \text{otherwise}
\end{cases}
]

LoRA dropout 仅作用于 A（输入侧），避免对主模型路径造成影响。

---

## **3. 什么时候使用 Dropout？**

### **✔ 适合使用（推荐）**

* 数据量少
* 小任务 + 大模型（极易过拟合）
* rank 较高（学习能力过强）

### **✘ 不推荐使用**

* 大规模 SFT（百万样本以上）
* 想最大化 LoRA 表达能力时

常见 dropout：

[
0.05 ~ 0.1
]

---

## **4. 推荐配置**

| 任务             | 推荐 dropout |
| -------------- | ---------- |
| 小数据（<10k）      | 0.1        |
| 中等数据（10k~100k） | 0.05       |
| 大数据（>100k）     | 0 or 0.01  |

---

## **5. 配置示例**

```python
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"]
)
```

---

## **6. 小结**

* LoRA dropout 用于防止 LoRA 分支过拟合
* 常用 0.05～0.1
* 大规模训练可设为 0
* 不影响主模型，安全易用

---

---

# **3.6 LoRA 的目标层选择（Attention 层）**

---

## **1. 概念讲解**

> LoRA 最常加在 Attention 的 Q、K、V、O 投影层，因为这些层负责模型的上下文建模能力，是任务迁移最关键的部分。

Transformer Attention 的核心线性层：

[
Q = W_q X,\quad K = W_k X, \quad V = W_v X,\quad O = W_o H
]

选择它们作为 LoRA target modules 可以最大化微调效果。

---

## **2. 为什么选择 Attention 层？**

### **（1）Attention 决定模型对输入的“注意力分布”**

任务特定能力（如对话风格、分类特征）本质上改变注意力模式。

### **（2）Attention 的参数量适中（适合插 LoRA）**

例如 LLaMA-7B 的 Q/K/V/O 总共只占全模型 <1%。

### **（3）改变它们即可显著影响生成行为**

如：

* 改善对话逻辑
* 改善语气风格
* 增强特定任务表达

---

## **3. 不同模型的 Attention 层命名**

| 模型              | 名称                                |
| --------------- | --------------------------------- |
| Llama2 / Llama3 | q_proj / k_proj / v_proj / o_proj |
| ChatGLM         | query_key_value                   |
| Qwen            | q_proj / v_proj（视具体版本）            |
| GPT-NeoX        | attention.query_key_value         |

---

## **4. 推荐的 target_modules 配置**

通用配置（Llama 系）：

```python
target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
```

Qwen2 推荐（k_proj 有时与 q_proj 共享）：

```python
target_modules=["q_proj", "v_proj"]
```

ChatGLM 推荐：

```python
target_modules=["query_key_value"]
```

---

## **5. 如何自动查看模型模块？**

```python
for name, module in model.named_modules():
    if "proj" in name.lower() or "attn" in name.lower():
        print(name)
```

---

## **6. 小结**

* 最推荐的 target modules → **Attention 层**
* Q/K/V/O 是最关键的目标层
* 不同模型名称不同，需要检查
* FFN 层也可以加入（增强效果）

---

# **3.7 PEFT 的模块化配置方法（Modular Configuration）**

---

## **1. 概念讲解**

PEFT（Parameter-Efficient Fine-Tuning）库为我们提供了模块化的配置方式，可以灵活选择：

* 插入何种微调方案（LoRA、Prefix-Tuning、IA3…）
* 添加到哪些层
* 各项超参数（rank、alpha、dropout…）

> 目标：让微调方式可组合、可复用、可配置。

通过 `LoraConfig`、`PrefixTuningConfig` 等，PEFT 将所有设定封装成模块，训练脚本只需简单引用即可。

---

## **2. 模块化配置的优势**

| 优势   | 说明               |
| ---- | ---------------- |
| 高复用性 | 配置文件可跨多个模型/任务复用  |
| 灵活性  | 能快速组合不同的 PEFT 技术 |
| 代码简洁 | 微调脚本变得更轻量        |
| 可叠加  | 可以多次堆叠不同 LoRA 模块 |

例如对一个模型可以先 LoRA 微调分类，再加一个 LoRA 微调指令。

---

## **3. PEFT 核心接口**

### **（1）配置类**

```python
from peft import LoraConfig
```

### **（2）构建可微调模型**

```python
from peft import get_peft_model

model = get_peft_model(base_model, config)
```

### **（3）保存 / 加载 PEFT adapter**

```python
model.save_pretrained("lora_adapter")
```

---

## **4. 配置示例：完整 PEFT + LoRA 配置**

```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM",   # 明确任务类型
)
```

常见 task_type：

* `CAUSAL_LM` ：自回归模型（LLaMA、Qwen）
* `SEQ_2_SEQ_LM` ：T5 这种
* `TOKEN_CLS` ：分类任务
* `SEQ_CLS` ：序列分类

---

## **5. 代码示例：模块化微调流程**

```python
from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-0.5B")

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, config)
```

---

## **6. 小结**

* PEFT 将微调方式模块化，可定义可复用配置
* 核心接口：`LoraConfig` + `get_peft_model`
* 可堆叠多个适配器
* 模块化适合构建大规模训练框架

---

---

# **3.8 微调时的学习率设定（Learning Rate）**

---

## **1. 概念讲解**

在 LoRA 微调中，学习率对训练效果影响极大。

由于**只训练 LoRA 参数，不训练原模型**，所以 LoRA 可以使用：

> **比全量微调更高的学习率**

例如 Llama2-7B 全量微调学习率需 ~2e-5
而 LoRA 可以用 2e-4 到 3e-4。

---

## **2. 理论分析**

LoRA 训练分为两部分：

* 主模型参数冻结：不会更新
* LoRA 分支参数（B/A）可训练

这些可训练参数是小矩阵，不会破坏原模型能力，因此允许：

[
\text{学习率高 5～10 倍}
]

---

## **3. 推荐的学习率范围**

| 任务              | 推荐学习率             |
| --------------- | ----------------- |
| 通用 SFT（几十 k 数据） | 1e-4 ～ 2e-4       |
| 大规模 SFT（百万数据）   | 5e-5 ～ 1e-4       |
| 小数据任务           | 2e-4 ～ 3e-4       |
| 大 rank（>16）     | 5e-5 ～ 1e-4（避免爆炸） |

> **最常用推荐： 2e-4**

---

## **4. Transformer 微调常用学习率调度器**

常见选择：

| 调度器                  | 描述        |
| -------------------- | --------- |
| linear               | 线性衰减（最常见） |
| cosine               | 余弦退火（高效）  |
| constant             | 不变        |
| cosine_with_restarts | 用于长期训练    |

---

## **5. 配置示例**

```python
training_args = TrainingArguments(
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
)
```

---

## **6. 小结**

* LoRA 微调可使用相对较高学习率
* 通常使用 1e-4 ～ 3e-4
* 推荐结合 cosine 或 linear 调度器
* 大 rank 要更小学习率避免过拟合

---

---

# **3.9 微调时的 Batch Size 设定**

---

## **1. 概念讲解**

batch size（批大小）影响：

* 模型训练稳定性
* 显存占用
* 梯度估计噪声

LoRA 微调通常更轻量，可使用相对较大的 batch size。

---

## **2. 理论分析**

梯度噪声与 batch size 关系：

[
\sigma^2 \propto \frac{1}{\text{batch size}}
]

batch 越大：

* 梯度越平滑
* 收敛更稳定
* 但显存占用增加

LoRA 的参数量小，因此**整体显存开销低，可以使用更大的 batch size**。

---

## **3. 推荐 batch size**

| 模型规模  | 推荐 batch size |
| ----- | ------------- |
| 1B 以内 | 32～128        |
| 7B    | 16～64         |
| 13B   | 8～32          |
| >30B  | 2～16          |

---

## **4. 如果显存不够怎么办？**

使用以下技术：

* **gradient_accumulation_steps**
* **fp16/bf16 训练**
* **gradient checkpointing**

例如：
想要有效 batch size=64，但显存只允许 8：

[
\text{accumulation} = 64 / 8 = 8
]

---

## **5. 配置示例**

```python
training_args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,   # 4 * 8 = 32
)
```

---

## **6. 小结**

* batch size 影响训练稳定性
* LoRA 微调可以使用相对较大的 batch
* 显存不足可用梯度累积
* 一般 batch=32 是不错的起点

---

# **3.10 微调时的梯度累积设置（Gradient Accumulation）**

---

## **1. 概念讲解**

梯度累积（Gradient Accumulation）是一种在显存有限时模拟**更大 batch size**的技术。

> **核心思想：不在每个 batch 都更新梯度，而是累积多个小 batch 的梯度，然后一次性更新。**

---

## **2. 数学原理**

假设目标是有效 batch size = (N)，但显存只能容纳 batch size = (B)，那么：

[
\text{accumulation steps} = \frac{N}{B}
]

梯度累积过程：

[
g = \frac{1}{K}\sum_{i=1}^{K} g_i
]

其中：

* (g_i) 是每个小 batch 的梯度
* (K) 是累积次数

最后一次性更新：

[
\theta = \theta - \eta g
]

---

## **3. 为什么 LoRA 微调特别需要梯度累积？**

因为：

* LoRA 微调通常使用较大的 batch 提升稳定性
* 显存限制经常无法一次性放下所有样本
* 长上下文模型（LLaMA、Qwen）输入长度大幅增加显存占用

因此梯度累积是 **LoRA 微调的标准必备方法**。

---

## **4. 推荐设置**

| GPU显存 | 建议 batch size | 建议 accumulate |
| ----- | ------------- | ------------- |
| 8GB   | 1～2           | 16～32         |
| 16GB  | 2～4           | 8～16          |
| 24GB  | 4～8           | 4～8           |
| 48GB  | 16            | 2～4           |

---

## **5. 配置示例**

```python
training_args = TrainingArguments(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,
)
```

---

## **6. 小结**

* 梯度累积用于显存有限情况下模拟大 batch
* 累积次数 = 有效 batch / 实际 batch
* LoRA 微调常需要配合此技术

---

# **3.11 微调时的混合精度（FP16 / BF16）配置**

---

## **1. 概念讲解**

混合精度训练使用：

* **FP16（半精度）**
* **BF16（脑浮点）**

来降低显存占用、加速计算。

由于 LoRA 只增加少量可训练参数，因此混合精度相当适合 LoRA 微调。

---

## **2. FP16 vs BF16 比较**

| 特性     | FP16          | BF16                  |
| ------ | ------------- | --------------------- |
| 范围     | 小（易溢出）        | 大（稳定）                 |
| 精度     | 较低            | 更高                    |
| GPU 支持 | 所有 Ampere 及以上 | Ampere（A100）、Ada、H100 |
| 推荐度    | 中             | ☆☆☆ 强烈推荐              |

---

## **3. 为什么 BF16 更好？**

因为 BF16 表示范围大，避免梯度爆炸和梯度下溢。

例如 FP16 表示范围：

[
6.1 \times 10^{-5} ~ 6.5\times 10^{4}
]

而 BF16：

[
3.9 \times 10^{-38} ~ 3.4 \times 10^{38}
]

适用于大模型训练的动态范围需求。

---

## **4. 配置示例**

### **FP16**

```python
training_args = TrainingArguments(
    fp16=True
)
```

### **BF16**

```python
training_args = TrainingArguments(
    bf16=True
)
```

---

## **5. 代码示例：完整设置**

```python
training_args = TrainingArguments(
    output_dir="output",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    bf16=True,                       # 推荐
    logging_steps=10,
)
```

---

## **6. 小结**

* FP16 / BF16 提升训练速度、减少显存
* BF16 更稳定，推荐优先使用
* 混合精度是 LoRA 微调的常用手段

---

---

# **3.12 微调时的显存管理（Gradient Checkpointing）**

---

## **1. 概念讲解**

Gradient Checkpointing 是一种显存优化技术：

> **不保存每层的前向激活，而是训练时反向传播重新计算前向，减少显存占用。**

显存换计算（时间变慢，但内存省很多）。

---

## **2. 原理分析**

普通训练保存所有激活：

[
\text{Memory} \propto \text{Layers}
]

而 Gradient Checkpointing 只保存关键节点：

[
\text{Memory} \propto \log(\text{Layers})
]

大幅降低显存占用。

---

## **3. 为什么 LoRA 微调适合启用？**

* LoRA 本身训练轻量，但上下文长度长（例如 4096）
* Transformer 的注意力显存开销非常高
* 在消费级 GPU（如 3090/4090）上特别需要开启

---

## **4. 启用方式**

### **HuggingFace Transformers**

```python
model.gradient_checkpointing_enable()
```

在 TrainingArguments 中：

```python
training_args = TrainingArguments(
    gradient_checkpointing=True
)
```

---

## **5. 优缺点**

| 优点             | 缺点              |
| -------------- | --------------- |
| 显存减少 20%～40%   | 训练速度略变慢（10～30%） |
| 可使用更长 context  | 需要更高计算量         |
| 可增大 batch size | 每 step 耗时增加     |

---

## **6. 配置示例（完整）**

```python
model.gradient_checkpointing_enable()

training_args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    gradient_checkpointing=True,
    bf16=True,
)
```

---

## **7. 小结**

* Gradient Checkpointing 是最有效的显存优化手段
* 用显存换时间，训练更稳定
* LoRA 微调非常建议启用（尤其是长文本任务）

---

# **3.13 微调时的训练稳定性监控**

---

## **1. 概念讲解**

微调大型语言模型（LLM）时，训练稳定性至关重要。
不稳定的训练会导致：

* Loss 振荡
* 梯度爆炸
* 模型退化（失语、重复输出）

LoRA 微调虽然比全量微调稳定，但仍需对训练过程进行监控，以确保质量和安全性。

---

## **2. 训练稳定性的关键指标**

### **（1）训练损失（Training Loss）**

最重要的指标。

正常趋势：

[
\text{Loss}(t) \downarrow
]

若出现：

* Loss 不下降
* Loss 震荡幅度 >20%
* Loss 过早降至极低值（过拟合）

都代表训练不健康。

---

### **（2）学习率曲线（LR Schedule）**

观察 lr_schedulers 是否正常衰减。
linear 或 cosine 策略通常应平稳下降。

---

### **（3）梯度范数（Gradient Norm）**

用于判断梯度爆炸：

[
||g||_2 \gg 1
]

如果 gradient norm 频繁飙升，需要：

* 减小学习率
* 启用梯度裁剪（gradient clipping）

---

### **（4）显存利用率（GPU Memory）**

注意：

* 不断增长 → 可能内存泄露
* 达到 100% → batch 设置过大

---

### **（5）模型输出质量**

可设置周期 evaluation：

* 输出是否完整
* 是否重复词语
* 是否出现乱序、乱码
* 是否出现“失语现象”（无意义重复）

---

## **3. 稳定性增强策略**

| 方法                          | 效果                 |
| --------------------------- | ------------------ |
| 减小学习率                       | 90% 的训练不稳定由学习率过大导致 |
| 增加 gradient_accumulation    | 稳定批数据分布            |
| 启用 gradient clipping（推荐1.0） | 避免梯度爆炸             |
| 使用 BF16                     | 提升训练稳定性            |
| LoRA dropout 增大到 0.1        | 降低过拟合              |
| 使用 cosine 调度                | 更平滑                |

---

## **4. 代码示例：如何记录训练稳定性**

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="output",
    logging_steps=20,
    save_steps=1000,
    evaluation_strategy="steps",
    eval_steps=500,
    gradient_clipping=1.0,          # 避免梯度爆炸
    report_to="tensorboard",        # 可视化
)
```

运行后可使用：

```bash
tensorboard --logdir output
```

进行稳定性监控。

---

## **5. 小结**

* 训练 Loss、梯度范数、LR 曲线是最重要指标
* 不稳定多由学习率过大引起
* BF16 + Gradient Clipping 是稳定训练的最佳组合
* 建议开启 TensorBoard 可视化监控

---

---

# **3.14 微调后 LoRA 权重合并方法（Merge LoRA Weights）**

---

## **1. 概念讲解**

LoRA 微调后，模型包含两部分：

* 主模型权重（冻结）
* LoRA 权重（可训练）

使用时有两种方式：

### **🚀 方法 1：推理时加载 LoRA Adapter（推荐）**

无需合并权重：

```python
model = AutoModel.from_pretrained(base)
model = PeftModel.from_pretrained(model, "lora_adapter")
```

### **🚀 方法 2：将 LoRA 合并到原模型（导出轻量模型）**

用于终端推理、部署、量化推理。

[
W' = W + \Delta W = W + \alpha \cdot \frac{1}{r} BA
]

---

## **2. 合并 LoRA 的理由**

| 使用场景               | 是否需要合并 |
| ------------------ | ------ |
| 部署到手机 / 边缘设备       | ✔ 必须合并 |
| 使用 GPTQ/INT4 量化推理  | ✔ 必须合并 |
| 多 LoRA 叠加 → 只用其中一个 | ✔ 建议合并 |
| 训练后继续 fine-tune    | ✘ 不用合并 |

---

## **3. 合并 LoRA 代码示例**

```python
from peft import PeftModel

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-0.5B")
model = PeftModel.from_pretrained(model, "lora_adapter")

model = model.merge_and_unload()
model.save_pretrained("merged_model")
```

此时得到的是一个**完全普通的模型**，可直接量化。

---

## **4. 如何验证权重已合并？**

```python
print(model)
```

若输出中已不存在 `lora` 字样，则表示合并成功。

---

## **5. 小结**

* LoRA 合并 = 原权重 + LoRA 更新
* 推理时可不合并，但部署时通常需要
* 合并后可保存并量化
* 使用 `merge_and_unload()` 最方便

---

---

# **3.15 微调后模型评估方法（Evaluation）**

---

## **1. 概念讲解**

微调完成后，需要通过多个维度评估模型效果：

* **语言建模质量（如 PPL）**
* **生成任务质量（如 BLEU、ROUGE）**
* **对话质量（人工评估 A/B Test）**
* **安全性评估**

---

## **2. 评估指标**

### **（1）困惑度（PPL，Perplexity）**

常用 LM 评估指标：

[
\text{PPL} = \exp(\text{Loss})
]

Loss 越小，PPL 越小 → 模型表现越好。

---

### **（2）BLEU（机器翻译/生成任务）**

衡量预测输出与参考答案的匹配度：

[
BLEU = BP \cdot \exp \left( \sum_{n} w_n \log p_n \right)
]

---

### **（3）ROUGE（摘要任务）**

ROUGE-L 关注最长公共子序列（LCS）。

---

### **（4）人工评估**

最可靠方式：

* 连贯性
* 信息性
* 风格一致性
* 错误率

---

## **3. 代码示例：计算 PPL**

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("merged_model")
tokenizer = AutoTokenizer.from_pretrained("merged_model")

text = "今天的天气非常适合去公园散步。"

inputs = tokenizer(text, return_tensors="pt")
labels = inputs["input_ids"]

with torch.no_grad():
    outputs = model(**inputs, labels=labels)
    loss = outputs.loss
    ppl = torch.exp(loss)

print("PPL:", ppl.item())
```

---

## **4. BLEU 评估示例**

使用 `sacrebleu`：

```python
!pip install sacrebleu

from sacrebleu import corpus_bleu

preds = ["我喜欢机器学习"]
refs = [["我喜欢机器学习"]]

print(corpus_bleu(preds, refs))
```

---

## **5. 人工评估方法（A/B Test）**

步骤：

1. 给多个 tester 统一任务
2. 提供生成 A（base 模型）和 B（微调模型）
3. 随机排序
4. 统计胜率

常用指标：

| 指标               | 说明       |
| ---------------- | -------- |
| 帮助性（Helpfulness） | 答案质量与实用性 |
| 安全性（Safety）      | 是否避免违禁内容 |
| 一致性（Consistency） | 是否保持稳定风格 |
| 正确性（Factuality）  | 是否事实正确   |

---

## **6. 小结**

* 评估覆盖定量（PPL/ BLEU）+ 定性（人工评价）
* PPL = exp(loss) 最常用
* BLEU、ROUGE 用于特定任务
* 人工评估最可靠
* LoRA 微调一般需要多方面评估才能确保效果

---